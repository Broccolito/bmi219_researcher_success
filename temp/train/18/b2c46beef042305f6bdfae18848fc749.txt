Pre-trained Models for Representation Learning; Representation Learning for Compositional Semantics; Sentence and Document Representation Learning; Ten Key Problems of Pre-trained Models: An Outlook of Representation Learning; OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models; Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer; KoLA: Carefully Benchmarking World Knowledge of Large Language Models; Arbitrary Few Parameters are Good Enough for Adapting Large-scale Pre-trained Language Models; Exploring Lottery Prompts for Pre-trained Language Models; Enhancing Chat Language Models by Scaling High-quality Instructional Conversations; WebCPM: Interactive Web Search for Chinese Long-form Question Answering; Tool Learning with Foundation Models; Parameter-efficient fine-tuning of large-scale pre-trained language models; Transcription between human-readable synthetic descriptions and machine-executable instructions: an application of the latest pre-training technology; Decoder Tuning: Efficient Language Understanding as Decoding; MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction; Few-shot Classification with Hypersphere Modeling of Prototypes; Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning; Improving Task Generalization via Unified Schema Prompt; Sparse Structure Search for Parameter-Efficient Tuning
