On the opportunities and risks of foundation models; Isolating sources of disentanglement in variational autoencoders; Alpaca: A strong, replicable instruction-following model; Inference Suboptimality in Variational Autoencoders; Scalable gradients for stochastic differential equations; Holistic evaluation of language models; Large language models can be strong differentially private learners; Neural sdes as infinite-dimensional gans; Stochastic runge-kutta accelerates langevin monte carlo and beyond; Scalable gradients and variational inference for stochastic differential equations; Infinitely deep bayesian neural networks with stochastic differential equations; When does preconditioning help or hurt generalization?; Exploiting programmatic behavior of llms: Dual-use through standard security attacks; Efficient and accurate gradients for neural sdes; Alpacafarm: A simulation framework for methods that learn from human feedback; Foundation models and fair use; When Does Differentially Private Learning Not Suffer in High Dimensions?; Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe; Exploring the limits of differentially private deep learning with group-wise clipping; A closer look at the calibration of differentially private learners
