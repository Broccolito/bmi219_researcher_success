DeepFlame: A deep learning empowered open-source platform for reacting flow simulations; Subspace decomposition based DNN algorithm for elliptic type multi-scale PDEs; Optimistic Estimate Uncovers the Potential of Nonlinear Models; Stochastic Modified Equations and Dynamics of Dropout Algorithm; Loss Spike in Training Neural Networks; Understanding the Initial Condensation of Convolutional Neural Networks; A Correction and Comments on “Multi-Scale Deep Neural Network (MscaleDNN) for Solving Poisson-Boltzmann Equation in Complex Domains. CiCP, 28 (5): 1970–2001, 2020”; Laplace-fPINNs: Laplace-based fractional physics-informed neural networks for solving forward and inverse problems of subdiffusion; Phase diagram of initial condensation for two-layer neural networks; A non-gradient method for solving elliptic partial differential equations with deep neural networks; Towards understanding the condensation of neural networks at initial training; Bayesian inversion with neural operator (bino) for modeling subdiffusion: Forward and inverse problems; Linear stability hypothesis and rank stratification for nonlinear models; An Upper Limit of Decaying Rate with Respect to Frequency in Linear Frequency Principle Model; Implicit regularization of dropout; Data-informed deep optimization; Embedding principle in depth for the loss landscape analysis of deep neural networks; An Experimental Comparison Between Temporal Difference and Residual Gradient with Neural Network Approximation; Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width; Limitation of characterizing implicit regularization by data-independent functions
