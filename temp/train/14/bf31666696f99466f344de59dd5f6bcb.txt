Optimistic Estimate Uncovers the Potential of Nonlinear Models; Towards understanding the condensation of neural networks at initial training; Empirical phase diagram for three-layer neural networks with infinite width; Linear stability hypothesis and rank stratification for nonlinear models; On the exact computation of linear frequency principle dynamics and its generalization; A multi-scale sampling method for accurate and robust deep neural network to predict combustion chemical kinetics; An Upper Limit of Decaying Rate with Respect to Frequency in Linear Frequency Principle Model; Data-informed deep optimization; Implicit bias in understanding deep learning for solving PDEs beyond Ritz-Galerkin method; Embedding principle in depth for the loss landscape analysis of deep neural networks; Embedding Principle: a hierarchical structure of loss landscape of deep neural networks; Limitation of characterizing implicit regularization by data-independent functions; Overview frequency principle/spectral bias in deep learning; A deep learning-based model reduction (DeePMR) method for simplifying chemical kinetics; Embedding principle of loss landscape of deep neural networks; Theory of the Frequency Principle for General Deep Neural Networks; MOD-Net: A machine learning approach via model-operator-data network for solving PDEs; An upper limit of decaying rate with respect to frequency in deep neural network; A linear frequency principle model to understand the absence of overfitting in neural networks; DLODE: a deep learning-based ODE solver for chemistry kinetics
