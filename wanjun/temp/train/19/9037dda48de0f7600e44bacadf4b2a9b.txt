Towards theoretically understanding why sgd generalizes better than adam in deep learning; How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective; The Barron Space and the Flow-Induced Function Spaces for Neural Network Models; Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't; Bispectrum inversion with application to multireference alignment; A priori estimates of the population risk for two-layer neural networks; A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics; A mean field analysis of deep resnet and beyond: Towards provably optimization via overparameterization from depth; Uniformly Accurate Machine Learning Based Hydrodynamic Models for Kinetic Equations; Model reduction with memory and the machine learning of dynamical systems; Machine learning from a continuous viewpoint, I; Modeling subgrid-scale force and divergence of heat flux of compressible isotropic turbulence by artificial neural network; Artificial neural network approach to large-eddy simulation of compressible isotropic turbulence; Rademacher complexity and the generalization error of residual networks; On linear stability of sgd and input-smoothness of neural networks; Heterogeneous multireference alignment for images with application to 2D classification in single particle reconstruction; Global convergence of gradient descent for deep linear residual networks; Globally convergent Levenberg-Marquardt method for phase retrieval; The Multiscale Structure of Neural Network Loss Functions: The Effect on Optimization and Origin; The slow deterioration of the generalization error of the random feature model
