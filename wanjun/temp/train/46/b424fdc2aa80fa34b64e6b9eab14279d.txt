Automatic chemical design using a data-driven continuous representation of molecules; Probabilistic backpropagation for scalable learning of bayesian neural networks; Grammar variational autoencoder; Minerva: Enabling low-power, highly-accurate deep neural network accelerators; Predictive entropy search for efficient global optimization of black-box functions; Gans for sequences of discrete elements with the gumbel-softmax distribution; Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning; Deep Gaussian processes for regression using approximate expectation propagation; Black-box alpha divergence minimization; Constrained Bayesian optimization for automatic chemical design using variational autoencoders; Predictive entropy search for multi-objective bayesian optimization; Deterministic variational inference for robust bayesian neural networks; Learning and policy search in stochastic dynamical systems with bayesian neural networks; Probabilistic matrix factorization with non-random missing data; Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space; Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control; A general framework for constrained Bayesian optimization using information-based search; Predictive entropy search for Bayesian optimization with unknown constraints; Stochastic expectation propagation; Collaborative gaussian processes for preference learning
