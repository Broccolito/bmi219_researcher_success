Efficient approximation of high-dimensional functions with neural networks; Non-convergence of stochastic gradient descent in the training of deep neural networks; A proof of convergence for gradient descent in the training of artificial neural networks for constant target functions; Landscape analysis for shallow neural networks: complete classification of critical points for affine target functions; Gradient descent provably escapes saddle points in the training of shallow ReLU networks; Magnetic and Exotic Anosov Hamiltonian Structures; Currents in Geometry and Analysis; The Moduli Space of Hyperbolic Surfaces, Analytic Teichm√ºller Theory, and the Pants Graph; An Introduction to Complex Dynamics and the Mandelbrot Set
