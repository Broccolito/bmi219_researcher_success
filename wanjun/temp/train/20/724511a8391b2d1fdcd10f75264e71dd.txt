Neural ordinary differential equations; Isolating Sources of Disentanglement in Variational Autoencoders; FFJORD: Free-form continuous dynamics for scalable reversible generative models; Latent odes for irregularly-sampled time series; Invertible residual networks; Fast patch-based style transfer of arbitrary style; Residual flows for invertible generative modeling; Scalable gradients for stochastic differential equations; Scalable reversible generative models with free-form continuous dynamics; Learning Neural Event Functions for Ordinary Differential Equations; Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization; Neural Spatio-Temporal Point Processes; Flow Matching for Generative Modeling; Scalable gradients and variational inference for stochastic differential equations; Theseus: A Library for Differentiable Nonlinear Optimization; “Hey, that’s not an ODE”: Faster ODE Adjoints via Seminorms; torchdiffeq, 2018; Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations; Neural networks with cheap differential operators; SUMO: Unbiased estimation of log marginal probability for latent variable models
