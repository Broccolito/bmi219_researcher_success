A proof of convergence for gradient descent in the training of artificial neural networks for constant target functions; Convergence analysis for gradient flows in the training of artificial neural networks with ReLU activation; A proof of convergence for stochastic gradient descent in the training of artificial neural networks with ReLU activation for constant target functions; Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with ReLU activation; On the existence of global minima and convergence analyses for gradient descent methods in the training of deep neural networks; A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear …; Convergence proof for stochastic gradient descent in the training of deep neural networks with ReLU activation for constant target functions; Convergence rates for empirical measures of Markov chains in dual and Wasserstein distances; On the existence of infinitely many realization functions of non-global local minima in the training of artificial neural networks with ReLU activation; Strong overall error analysis for the training of artificial neural networks via random initializations; Convergence to good non-optimal critical points in the training of neural networks: Gradient descent optimization with one random initialization overcomes all bad non-global …; Normalized gradient flow optimization in the training of ReLU artificial neural networks; A proof of the corrected Sister Beiter cyclotomic coefficient conjecture inspired by Zhao and Zhang; Deep neural network approximation of composite functions without the curse of dimensionality; Algorithmically Designed Artificial Neural Networks (ADANNs): Higher order deep operator learning for parametric partial differential equations
